{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e2d383a",
   "metadata": {},
   "source": [
    "# Define StateSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a17027ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import Annotated, Sequence, TypedDict\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ddd60",
   "metadata": {},
   "source": [
    "## Initialize a new Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27e6456a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ID: 537b2a44-1dac-4d06-8792-fbb01832c03c\n"
     ]
    }
   ],
   "source": [
    "from utils import ChatHistoryManager\n",
    "chat: ChatHistoryManager = ChatHistoryManager.new_chat()\n",
    "print(f\"Chat ID:\", chat.chat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e8681",
   "metadata": {},
   "source": [
    "## Initialize the RAG System\n",
    "Using `TextDocumentRAG()` from `intellitube.rag` module automatically initializes `Qdrant` client as Vector Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcf9dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-08 16:20:23.455\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mintellitube.rag\u001b[0m:\u001b[36minit_vector_store\u001b[0m:\u001b[36m66\u001b[0m - \u001b[34m\u001b[1mCreaing Client...\u001b[0m\n",
      "\u001b[32m2025-07-08 16:20:23.465\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mintellitube.rag\u001b[0m:\u001b[36minit_vector_store\u001b[0m:\u001b[36m73\u001b[0m - \u001b[34m\u001b[1mCreaing vector store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from intellitube.rag import TextDocumentRAG\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_rag = TextDocumentRAG(\n",
    "    path_on_disk=chat.chat_dirpath,\n",
    "    collection_path_on_disk=os.path.join(chat.chat_dirpath, \"collection\"),\n",
    "    collection_name=chat.chat_id,\n",
    ")\n",
    "\n",
    "def add_to_vdb(docuemnts: List[Document]) -> None:\n",
    "    # convert to a list of document(s) if not already!\n",
    "    if type(docuemnts) == Document:\n",
    "        docuemnts = [docuemnts]\n",
    "    \n",
    "    document_rag.add_documents(\n",
    "        docuemnts, split_text=True,\n",
    "        split_config={\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 128\n",
    "        },\n",
    "        skip_if_collection_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0781f4",
   "metadata": {},
   "source": [
    "## Create Document Loader Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8ebb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e792db4",
   "metadata": {},
   "source": [
    "### 1. Add YouTube Videos to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd262d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    YTContentData,\n",
    "    webvtt_2_str,\n",
    "    download_youtube_audio_or_transcript,\n",
    ")\n",
    "\n",
    "test_url = \"https://www.youtube.com/watch?v=W3I3kAg2J7w&t=231s\"\n",
    "\n",
    "@tool\n",
    "def load_youtube_transcript(youtube_url: str) -> str:\n",
    "    \"\"\"Load the given YouTube video's transcript to the vector database.\n",
    "    It is required to answer user-queries based on the the Transcript context.\"\"\"\n",
    "    \n",
    "    # download the youtube transcript (or audio if transcript not available)\n",
    "    yt_video_data: YTContentData = download_youtube_audio_or_transcript(\n",
    "        video_url=youtube_url,\n",
    "    )\n",
    "\n",
    "    # convert the WEBVTT format trancript to a plain text string\n",
    "    vtt_str = webvtt_2_str(vtt_file_path=yt_video_data.transcript_path)\n",
    "    \n",
    "    print(vtt_str[:100])    # print first 100 characters\n",
    "\n",
    "    # add the transcript-string to the vector database\n",
    "    add_to_vdb(Document(vtt_str))\n",
    "    return \"YouTube Video Transcript has been loaded successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac44794",
   "metadata": {},
   "source": [
    "### 2. Add PDF/Text Documents to the Vector Dataabse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33961ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "@tool\n",
    "def load_document(document_path: str) -> str:\n",
    "    \"\"\"Load the given Document's content to the vector database.\n",
    "    It is required to answer user-queries based on the the Document context.\"\"\"\n",
    "    \n",
    "    ext = os.path.splitext(document_path)[1][1:].lower()\n",
    "    documents: List[Document]\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        documents = PyPDFLoader(document_path).load()\n",
    "    elif ext == 'txt':\n",
    "        with open(document_path, 'r') as file:\n",
    "            documents = [Document(\n",
    "                page_content=file.read(),\n",
    "                metadata={ \"source\": document_path }\n",
    "            )]\n",
    "    else:\n",
    "        return f\"Unsupported filetype: {ext}!\"\n",
    "    \n",
    "    add_to_vdb(documents)\n",
    "    return \"The document has been loaded successfully!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8cdd0",
   "metadata": {},
   "source": [
    "### 3. Add WebPages as Documents to the Vector Dataabse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fccbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "@tool\n",
    "def load_webpage(webpage_url: str) -> str:\n",
    "    \"\"\"Load the given WebSite's content to the vector database.\n",
    "    It is required to answer user-queries based on the the WebPage's context.\"\"\"\n",
    "\n",
    "    add_to_vdb(WebBaseLoader(webpage_url).load())\n",
    "    return \"The webpage has been loaded successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268bf2a",
   "metadata": {},
   "source": [
    "### Pass the Query Tool\n",
    "This is a function to be called by the Agent if none of the other tools can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def pass_user_query(user_query: str) -> None:\n",
    "    \"\"\"Use this tool when none of the other tools are useful.\"\"\"\n",
    "    print(f\"Passes User Query: {user_query}\")\n",
    "    return user_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1f488",
   "metadata": {},
   "source": [
    "#### Finally, compile a list of the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e63e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='load_youtube_transcript', description=\"Load the given YouTube video's transcript to the vector database.\\n    It is required to answer user-queries based on the the Transcript context.\", args_schema=<class 'langchain_core.utils.pydantic.load_youtube_transcript'>, func=<function load_youtube_transcript at 0x7f02c8493c40>),\n",
      " StructuredTool(name='load_document', description=\"Load the given Document's content to the vector database.\\n    It is required to answer user-queries based on the the Document context.\", args_schema=<class 'langchain_core.utils.pydantic.load_document'>, func=<function load_document at 0x7f02c84f2200>),\n",
      " StructuredTool(name='load_webpage', description=\"Load the given WebSite's content to the vector database.\\n    It is required to answer user-queries based on the the WebPage's context.\", args_schema=<class 'langchain_core.utils.pydantic.load_webpage'>, func=<function load_webpage at 0x7f02c8493600>),\n",
      " StructuredTool(name='pass_user_query', description='Use this tool when none of the other tools are useful.', args_schema=<class 'langchain_core.utils.pydantic.pass_user_query'>, func=<function pass_user_query at 0x7f0271bfa7a0>)]\n"
     ]
    }
   ],
   "source": [
    "tools = [load_youtube_transcript, load_document, load_webpage, pass_user_query]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0d559",
   "metadata": {},
   "source": [
    "## Choose an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c4a67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "# from langchain.chat_models import init_chat_model\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def select_llm(\n",
    "    model_provider: Literal['openai', 'groq', 'nvidia', 'google', 'ollama'],\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 0.0,\n",
    ") -> BaseChatModel:\n",
    "    if model_provider == 'openai':\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        return ChatOpenAI(model=model_name or \"gpt-4o-mini\", temperature=temperature)\n",
    "    elif model_provider == 'groq':\n",
    "        from langchain_groq import ChatGroq\n",
    "        return ChatGroq(model=model_name or \"llama-3.3-70b-versatile\", temperature=temperature)\n",
    "    elif model_provider == 'nvidia':\n",
    "        from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "        # return ChatNVIDIA(model=model_name or \"mistralai/mistral-small-24b-instruct\", temperature=temperature)\n",
    "        return ChatNVIDIA(model=model_name or \"nvidia/llama-3.1-nemotron-51b-instruct\", temperature=temperature)\n",
    "    elif model_provider == 'google':\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "        return ChatGoogleGenerativeAI(model=model_name or \"gemini-2.0-flash\", temperature=temperature)\n",
    "    elif model_provider == 'ollama':\n",
    "        from langchain_ollama import ChatOllama\n",
    "        return ChatOllama(model=model_name or \"granite3.3:8b\", temperature=temperature)\n",
    "    \n",
    "    raise ValueError(f\"Invalid model_provider: {model_provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f27bf8",
   "metadata": {},
   "source": [
    "#### Test the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1677ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LLM = False\n",
    "llm = select_llm(model_provider='groq')\n",
    "\n",
    "if TEST_LLM:\n",
    "    resp = llm.invoke(\"What is superiority complex? Respond with a nicely structured & formatted answer!\")\n",
    "    print(resp)\n",
    "    \n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(resp.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa6ca5",
   "metadata": {},
   "source": [
    "## Create a Router\n",
    "This router will decide if the user has provided any Document/Website URL/YouTube URL. Depending on the type of URL it will call a function to load the document or just redirect the query to a RAG Agent for direct response generation if no URL is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db004a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "def document_loader_router(user_query: str):\n",
    "    tools_llm = llm.bind_tools(tools=tools)\n",
    "    system_prompt = SystemMessage(\n",
    "f\"\"\"You are a very helpful assistant. You have access to {len(tools)} tools.\n",
    "\n",
    "Here is when to use which tool:\n",
    "    - load_youtube_transcript: To load an YouTube Video's transcript\n",
    "    - load_document: To load a Text/PDF document (can be a local path)\n",
    "    - load_webpage: To laod an WebPage\n",
    "    - pass_user_query: When you find no URL/Path in the user query\n",
    "\n",
    "Here is the user query: {user_query}\n",
    "\n",
    "Observe the user query and if you see any URL/Path of a file/document/YT Video/Website use the necessary tool to load it.\n",
    "If you see no URL then just use the `pass_user_query` tool to pass the query to the next Agent.\n",
    "    \"\"\")\n",
    "\n",
    "    resp = tools_llm.invoke([system_prompt, HumanMessage(user_query)])\n",
    "    pprint(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1abffe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Test 1 ***************\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'taqwbp2e8', 'function': {'arguments': '{\"webpage_url\":\"https://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\"}', 'name': 'load_webpage'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 736, 'total_tokens': 776, 'completion_time': 0.145454545, 'prompt_time': 0.071017883, 'queue_time': 0.052987970999999995, 'total_time': 0.216472428}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--755d691e-6034-4e6b-b306-b6ef89f71375-0' tool_calls=[{'name': 'load_webpage', 'args': {'webpage_url': 'https://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct'}, 'id': 'taqwbp2e8', 'type': 'tool_call'}] usage_metadata={'input_tokens': 736, 'output_tokens': 40, 'total_tokens': 776}\n",
      "\n",
      "\n",
      "\n",
      "--------------- Test 2 ***************\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'jffmx4jhf', 'function': {'arguments': '{\"youtube_url\":\"https://www.youtube.com/watch?v=W3I3kAg2J7w\\\\u0026t=231s\"}', 'name': 'load_youtube_transcript'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 730, 'total_tokens': 769, 'completion_time': 0.141818182, 'prompt_time': 0.074127359, 'queue_time': 0.052503675, 'total_time': 0.215945541}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--7a7ca3ed-c20e-4cba-8875-729199a77967-0' tool_calls=[{'name': 'load_youtube_transcript', 'args': {'youtube_url': 'https://www.youtube.com/watch?v=W3I3kAg2J7w&t=231s'}, 'id': 'jffmx4jhf', 'type': 'tool_call'}] usage_metadata={'input_tokens': 730, 'output_tokens': 39, 'total_tokens': 769}\n",
      "\n",
      "\n",
      "\n",
      "--------------- Test 3 ***************\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'knp5m27ns', 'function': {'arguments': '{\"document_path\":\"~/data.json\"}', 'name': 'load_document'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 694, 'total_tokens': 711, 'completion_time': 0.061818182, 'prompt_time': 0.045327938, 'queue_time': 0.051898602, 'total_time': 0.10714612}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--ccd6b679-8cf5-42e1-bdb6-ce4ca48826d1-0' tool_calls=[{'name': 'load_document', 'args': {'document_path': '~/data.json'}, 'id': 'knp5m27ns', 'type': 'tool_call'}] usage_metadata={'input_tokens': 694, 'output_tokens': 17, 'total_tokens': 711}\n",
      "\n",
      "\n",
      "\n",
      "--------------- Test 4 ***************\n",
      "\n",
      "content='' additional_kwargs={'tool_calls': [{'id': 'j9jh4q7ra', 'function': {'arguments': '{\"user_query\":\"Why oranges are red and violates are blue?\"}', 'name': 'pass_user_query'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 693, 'total_tokens': 717, 'completion_time': 0.087272727, 'prompt_time': 0.042180892, 'queue_time': 0.052847488, 'total_time': 0.129453619}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_3f3b593e33', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--649f2124-b8f2-4ac2-ba6e-c17049e4c13d-0' tool_calls=[{'name': 'pass_user_query', 'args': {'user_query': 'Why oranges are red and violates are blue?'}, 'id': 'j9jh4q7ra', 'type': 'tool_call'}] usage_metadata={'input_tokens': 693, 'output_tokens': 24, 'total_tokens': 717}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEST_DOC_LOAD_ROUTER = True\n",
    "\n",
    "if TEST_DOC_LOAD_ROUTER:\n",
    "    queries = [\n",
    "        # Website URL\n",
    "        \"What is the model name mentioned?\\nhttps://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\",\n",
    "        # YouTube URL\n",
    "        \"What do you see here?\\nhttps://www.youtube.com/watch?v=W3I3kAg2J7w&t=231s\",\n",
    "        # Local Document Path\n",
    "        \"Summarize this document: ~/data.json\",\n",
    "        # Just a normal Query\n",
    "        \"Why oranges are red and violates are blue?\"\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        print(\"-\"*15, f\"Test {i + 1}\", \"*\"*15, end='\\n\\n')\n",
    "        response = document_loader_router(user_query=query)\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
