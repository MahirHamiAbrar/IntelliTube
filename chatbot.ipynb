{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637ddd60",
   "metadata": {},
   "source": [
    "## Initialize a new Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ChatHistoryManager\n",
    "chat: ChatHistoryManager = ChatHistoryManager.new_chat()\n",
    "print(f\"Chat ID:\", chat.chat_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e8681",
   "metadata": {},
   "source": [
    "## Initialize the RAG System\n",
    "Using `TextDocumentRAG()` from `intellitube.rag` module automatically initializes `Qdrant` client as Vector Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from intellitube.rag import TextDocumentRAG\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "document_rag = TextDocumentRAG(\n",
    "    path_on_disk=chat.chat_dirpath,\n",
    "    collection_path_on_disk=os.path.join(chat.chat_dirpath, \"collection\"),\n",
    "    collection_name=chat.chat_id,\n",
    ")\n",
    "\n",
    "def add_to_vdb(docuemnts: List[Document]) -> None:\n",
    "    # convert to a list of document(s) if not already!\n",
    "    if type(docuemnts) == Document:\n",
    "        docuemnts = [docuemnts]\n",
    "    \n",
    "    document_rag.add_documents(\n",
    "        docuemnts, split_text=True,\n",
    "        split_config={\n",
    "            \"chunk_size\": 512,\n",
    "            \"chunk_overlap\": 128\n",
    "        },\n",
    "        skip_if_collection_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0781f4",
   "metadata": {},
   "source": [
    "## Create Document Loader Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e792db4",
   "metadata": {},
   "source": [
    "### 1. Add YouTube Videos to the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd262d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    YTContentData,\n",
    "    webvtt_2_str,\n",
    "    download_youtube_audio_or_transcript,\n",
    ")\n",
    "\n",
    "test_url = \"https://www.youtube.com/watch?v=W3I3kAg2J7w&t=231s\"\n",
    "\n",
    "@tool\n",
    "def load_youtube_transcript(youtube_url: str) -> str:\n",
    "    \"\"\"Load the given YouTube video's transcript to the vector database.\n",
    "    It is required to answer user-queries based on the the Transcript context.\"\"\"\n",
    "\n",
    "    print(\"Loading Youtube Transcript...\")\n",
    "    \n",
    "    # download the youtube transcript (or audio if transcript not available)\n",
    "    yt_video_data: YTContentData = download_youtube_audio_or_transcript(\n",
    "        video_url=youtube_url,\n",
    "    )\n",
    "\n",
    "    # convert the WEBVTT format trancript to a plain text string\n",
    "    vtt_str = webvtt_2_str(vtt_file_path=yt_video_data.transcript_path)\n",
    "    \n",
    "    print(vtt_str[:100])    # print first 100 characters\n",
    "\n",
    "    # add the transcript-string to the vector database\n",
    "    add_to_vdb(Document(vtt_str))\n",
    "    return \"YouTube Video Transcript has been loaded successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac44794",
   "metadata": {},
   "source": [
    "### 2. Add PDF/Text Documents to the Vector Dataabse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33961ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "@tool\n",
    "def load_document(document_path: str) -> str:\n",
    "    \"\"\"Load the given Document's content to the vector database.\n",
    "    It is required to answer user-queries based on the the Document context.\"\"\"\n",
    "    print(\"Loading Document...\")\n",
    "    \n",
    "    ext = os.path.splitext(document_path)[1][1:].lower()\n",
    "    documents: List[Document]\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        documents = PyPDFLoader(document_path).load()\n",
    "    elif ext == 'txt':\n",
    "        with open(document_path, 'r') as file:\n",
    "            documents = [Document(\n",
    "                page_content=file.read(),\n",
    "                metadata={ \"source\": document_path }\n",
    "            )]\n",
    "    else:\n",
    "        return f\"Unsupported filetype: {ext}!\"\n",
    "    \n",
    "    add_to_vdb(documents)\n",
    "    return \"The document has been loaded successfully!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af8cdd0",
   "metadata": {},
   "source": [
    "### 3. Add WebPages as Documents to the Vector Dataabse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08fccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "@tool\n",
    "def load_webpage(webpage_url: str) -> str:\n",
    "    \"\"\"Load the given WebSite's content to the vector database.\n",
    "    It is required to answer user-queries based on the the WebPage's context.\"\"\"\n",
    "\n",
    "    add_to_vdb(WebBaseLoader(webpage_url).load())\n",
    "    print(\"Loading Webpage...\")\n",
    "    return \"The webpage has been loaded successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a268bf2a",
   "metadata": {},
   "source": [
    "### Pass the Query Tool\n",
    "This is a function to be called by the Agent if none of the other tools can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def pass_user_query(user_query: str) -> None:\n",
    "    \"\"\"Use this tool when none of the other tools are useful.\"\"\"\n",
    "    print(f\"Passes User Query: {user_query}\")\n",
    "    return f\"User: {user_query}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1f488",
   "metadata": {},
   "source": [
    "#### Finally, compile a list of the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e63e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_loader_tools = [load_youtube_transcript, load_document, load_webpage, pass_user_query]\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(document_loader_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b0d559",
   "metadata": {},
   "source": [
    "## Choose an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4a67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "# from langchain.chat_models import init_chat_model\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def select_llm(\n",
    "    model_provider: Literal['openai', 'groq', 'nvidia', 'google', 'ollama'],\n",
    "    model_name: Optional[str] = None,\n",
    "    temperature: float = 0.0,\n",
    ") -> BaseChatModel:\n",
    "    if model_provider == 'openai':\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        return ChatOpenAI(model=model_name or \"gpt-4o-mini\", temperature=temperature)\n",
    "    elif model_provider == 'groq':\n",
    "        from langchain_groq import ChatGroq\n",
    "        return ChatGroq(model=model_name or \"llama-3.3-70b-versatile\", temperature=temperature)\n",
    "    elif model_provider == 'nvidia':\n",
    "        from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "        # return ChatNVIDIA(model=model_name or \"mistralai/mistral-small-24b-instruct\", temperature=temperature)\n",
    "        return ChatNVIDIA(model=model_name or \"nvidia/llama-3.1-nemotron-51b-instruct\", temperature=temperature)\n",
    "    elif model_provider == 'google':\n",
    "        from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "        return ChatGoogleGenerativeAI(model=model_name or \"gemini-2.0-flash\", temperature=temperature)\n",
    "    elif model_provider == 'ollama':\n",
    "        from langchain_ollama import ChatOllama\n",
    "        # return ChatOllama(model=model_name or \"granite3.3:8b\", temperature=temperature)\n",
    "        return ChatOllama(model=model_name or \"llama3.2:3b\", temperature=temperature)\n",
    "    \n",
    "    raise ValueError(f\"Invalid model_provider: {model_provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f27bf8",
   "metadata": {},
   "source": [
    "#### Test the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677ec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LLM = False\n",
    "llm = select_llm(model_provider='groq')\n",
    "# llm = select_llm(model_provider='ollama')\n",
    "\n",
    "if TEST_LLM:\n",
    "    resp = llm.invoke(\"What is superiority complex? Respond with a nicely structured & formatted answer!\")\n",
    "    print(resp)\n",
    "    \n",
    "    from IPython.display import display, Markdown\n",
    "    display(Markdown(resp.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a23b9b",
   "metadata": {},
   "source": [
    "## Define State Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02d385",
   "metadata": {},
   "source": [
    "### Messages State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing_extensions import Annotated, Sequence, TypedDict\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa6ca5",
   "metadata": {},
   "source": [
    "## Create Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f4f00",
   "metadata": {},
   "source": [
    "### Router Node\n",
    "This router will decide if the user has provided any Document/Website URL/YouTube URL. Depending on the type of URL it will call a function to load the document or just redirect the query to a RAG Agent for direct response generation if no URL is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db004a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, SystemMessage\n",
    "\n",
    "\n",
    "def router_agent_node(state: MessagesState) -> MessagesState:\n",
    "    user_query: str = state[\"messages\"][-1].content\n",
    "    tools_llm = llm.bind_tools(tools=document_loader_tools)\n",
    "    \n",
    "    system_prompt = SystemMessage(\n",
    "f\"\"\"You are a very helpful assistant. You have access to {len(document_loader_tools)} tools.\n",
    "\n",
    "Here is when to use which tool:\n",
    "    - load_youtube_transcript: To load an YouTube Video's transcript\n",
    "    - load_document: To load a Text/PDF document (can be a local path)\n",
    "    - load_webpage: To laod an WebPage\n",
    "    - pass_user_query: When you find no URL/Path in the user query\n",
    "\n",
    "Here is the user query: {user_query}\n",
    "\n",
    "Observe the user query and if you see any URL/Path of a file/document/YT Video/Website use the necessary tool to load it.\n",
    "If you see no URL then just use the `pass_user_query` tool to pass the query to the next Agent.\n",
    "\n",
    "You MUST use ONE of the above tools. DO NOT generate any extra text beyond what's instructed.\n",
    "    \"\"\")\n",
    "\n",
    "    ai_msg: AIMessage = tools_llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    # Validate id the ai_msg has tool calls (IT MUST)\n",
    "    # ...\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abffe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DOC_LOAD_ROUTER = False\n",
    "\n",
    "if TEST_DOC_LOAD_ROUTER:\n",
    "    queries = [\n",
    "        # Website URL\n",
    "        \"What is the model name mentioned?\\nhttps://build.nvidia.com/nvidia/llama-3_1-nemotron-51b-instruct\",\n",
    "        # YouTube URL\n",
    "        \"What do you see here?\\nhttps://www.youtube.com/watch?v=W3I3kAg2J7w&t=231s\",\n",
    "        # Local Document Path\n",
    "        \"Summarize this document: ~/data.json\",\n",
    "        # Just a normal Query\n",
    "        \"Why oranges are red and violates are blue?\"\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(queries):\n",
    "        print(\"-\"*15, f\"Test {i + 1}\", \"*\"*15, end='\\n\\n')\n",
    "        response = router_agent_node(user_query=query)\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4028b",
   "metadata": {},
   "source": [
    "### Chat Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07390d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_agent_node(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"A Chat Agent\"\"\"\n",
    "    system_prompt = SystemMessage(\"\"\"You are IntelliTube AI, a smart research parter for the user.\"\"\")\n",
    "    ai_msg: AIMessage = llm.invoke([system_prompt] + state[\"messages\"])\n",
    "    return {\"messages\": [ai_msg]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af9671",
   "metadata": {},
   "source": [
    "## Create the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "\n",
    "graph = (\n",
    "    StateGraph(state_schema=MessagesState)\n",
    "    .add_node(\"router_agent\", router_agent_node)\n",
    "    .add_node(\"tools\", ToolNode(tools=document_loader_tools))\n",
    "    .add_node(\"chat_agent\", chat_agent_node)\n",
    "    .add_edge(START, \"router_agent\")\n",
    "    .add_edge(\"router_agent\", \"tools\")\n",
    "    .add_edge(\"tools\", \"chat_agent\")\n",
    "    .add_edge(\"chat_agent\", END)\n",
    ")\n",
    "\n",
    "agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55daa84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(agent.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf9978",
   "metadata": {},
   "source": [
    "## Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb496ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "def chat_loop() -> None:\n",
    "    usr_msg: str = input(\">> \").strip()\n",
    "\n",
    "    while usr_msg.lower() != \"/exit\":\n",
    "        usr_msg = HumanMessage(usr_msg)\n",
    "        chat.add_message(usr_msg)\n",
    "        chat.chat_messages = agent.invoke({\"messages\": chat.chat_messages})\n",
    "        ai_msg: AIMessage = chat.chat_messages[-1]\n",
    "        ai_msg.pretty_print()\n",
    "        usr_msg: str = input(\">> \").strip()\n",
    "    chat.end_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intellitube",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
